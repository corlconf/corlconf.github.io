{"0": {
    "doc": "All Papers",
    "title": "All Papers",
    "content": ". | Deep Phase Correlation for End-to-End Heterogeneous Sensor Measurements Matching | Learning Arbitrary-Goal Fabric Folding with One Hour of Real Robot Experience | Robust Policies via Mid-Level Visual Representations: An Experimental Study in Manipulation and Navigation | Towards Autonomous Eye Surgery by Combining Deep Imitation Learning with Optimal Control | . ",
    "url": "http://localhost:4000/all",
    "relUrl": "/all"
  },"1": {
    "doc": "Home",
    "title": "ðŸ“„ CoRL Paper Explorer",
    "content": "Welcome to the Corl 2020 Paper Explorer. | Search for a paper above (using any terms in the author, abstract or title). | See all papers that will be presented on a particular day to the left. | . ",
    "url": "http://localhost:4000/#-corl-paper-explorer",
    "relUrl": "/#-corl-paper-explorer"
  },"2": {
    "doc": "Home",
    "title": "Home",
    "content": ". ",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },"3": {
    "doc": "Monday",
    "title": "Monday",
    "content": " ",
    "url": "http://localhost:4000/monday",
    "relUrl": "/monday"
  },"4": {
    "doc": "Learning Arbitrary-Goal Fabric Folding with One Hour of Real Robot Experience",
    "title": "Learning Arbitrary-Goal Fabric Folding with One Hour of Real Robot Experience",
    "content": "Paper PDF . Authors . Robert Lee (Queensland University of Technology)*; Daniel Ward (The University of Queensland); Vibhavari Dasagi (Queensland University of Technology); Akansel Cosgun (Monash University); Juxi Leitner (QUT); Peter Corke (Queensland University of Technology) . Interactive Session . 2020-11-18, 11:50 - 12:20 PST | PheedLoop Session . Abstract . Manipulating deformable objects, such as fabric, is a long standing problem in robotics, with state estimation and control posing a significant challenge for traditional methods. In this paper, we show that it is possible to learn fabric folding skills in only an hour of self-supervised real robot experience, without human supervision or simulation. Our approach relies on fully convolutional networks and the manipulation of visual inputs to exploit learned features, allowing us to create an expressive goal-conditioned pick and place policy that can be trained efficiently with real world robot data only. Folding skills are learned with only a sparse reward function and thus do not require reward function engineering, merely an image of the goal configuration. We demonstrate our method on a set of towel-folding tasks, and show that our approach is able to discover sequential folding strategies, purely from trial-and-error. We achieve state-of-the-art results without the need for demonstrations or simulation, used in prior approaches. Video . Reviews and Rebuttal . Reviews &amp; Rebuttal . ",
    "url": "http://localhost:4000/paper_518/",
    "relUrl": "/paper_518/"
  },"5": {
    "doc": "Robust Policies via Mid-Level Visual Representations: An Experimental Study in Manipulation and Navigation",
    "title": "Robust Policies via Mid-Level Visual Representations: An Experimental Study in Manipulation and Navigation",
    "content": "Paper PDF Supplemental Code . Authors . Bryan Chen (UC Berkeley)*; Alexander Sax (UC Berkeley); Francis Lewis (Stanford); Iro Armeni (Stanford University); Silvio Savarese (Stanford University); Amir Zamir (Swiss Federal Institute of Technology (EPFL)); Jitendra Malik (University of California at Berkeley); Lerrel Pinto (NYU/Berkeley) . Interactive Session . 2020-11-16, 11:10 - 11:40 PST | PheedLoop Session . Abstract . Vision-based robotics often factors the control loop into separate components for perception and control. Conventional perception components usually extract hand-engineered features from the visual input that are then used by the control component in an explicit manner. In contrast, recent advances in deep RL make it possible to learn these features end-to-end during training, but the final result is often brittle, fails unexpectedly under minuscule visual shifts, and comes with a high sample complexity cost. In this work, we study the effects of using mid-level visual representations asynchronously trained for traditional computer vision objectives as a generic and easy-to-decode perceptual state in an end-to-end RL framework. We show that the invariances provided by the mid-level representations aid generalization, improve sample complexity, and lead to a higher final performance. Compared to the alternative approaches for incorporating invariances, such as domain randomization, using asynchronously trained mid-level representations scale better to harder problems and larger domain shifts, and consequently, successfully trains policies for tasks where domain randomization or learning-from-scratch failed. Our experimental findings are reported on manipulation and navigation tasks using real robots as well as simulations. Video . Reviews and Rebuttal . Reviews &amp; Rebuttal . ",
    "url": "http://localhost:4000/paper_526/",
    "relUrl": "/paper_526/"
  },"6": {
    "doc": "Towards Autonomous Eye Surgery by Combining Deep Imitation Learning with Optimal Control",
    "title": "Towards Autonomous Eye Surgery by Combining Deep Imitation Learning with Optimal Control",
    "content": "Paper PDF . Authors . Ji Woong Kim (Johns Hopkins University)*; Peiyao Zhang (Johns Hopkins University); Peter Gehlbach (Johns Hopkins Hospital,); Iulian Iordachita (The Johns Hopkins University); Marin Kobilarov (Johns Hopkins University) . Interactive Session . 2020-11-18, 11:10 - 11:40 PST | PheedLoop Session . Abstract . During retinal microsurgery, precise manipulation of the delicate retinal tissue is required for positive surgical outcome. However, accurate manipulation and navigation of surgical tools remain difficult due to a constrained workspace and the top-down view during the surgery, which limits the surgeonâ€™s ability to estimate depth. To alleviate such difficulty, we propose to automate the tool-navigation task by learning to predict relative goal position on the retinal surface from the current tool-tip position. Given an estimated target on the retina, we generate an optimal trajectory leading to the predicted goal while imposing safety-related physical constraints aimed to minimize tissue damage. As an extended task, we generate goal predictions to various points across the retina to localize eye geometry and further generate safe trajectories within the estimated confines. Through experiments in both simulation and with several eye phantoms, we demonstrate that our framework can permit navigation to various points on the retina within 0.089mm and 0.118mm in xy error which is less than the humanâ€™s surgeon mean tremor at the tool-tip of 0.180mm. All safety constraints were fulfilled and the algorithm was robust to previously unseen eyes as well as unseen objects in the scene. Live video demonstration is available here: https://youtu.be/n5j5jCCelXk . Video . Reviews and Rebuttal . Reviews &amp; Rebuttal . ",
    "url": "http://localhost:4000/paper_528/",
    "relUrl": "/paper_528/"
  },"7": {
    "doc": "Deep Phase Correlation for End-to-End Heterogeneous Sensor Measurements Matching",
    "title": "Deep Phase Correlation for End-to-End Heterogeneous Sensor Measurements Matching",
    "content": "Paper PDF Code . Authors . Zexi Chen (Zhejiang University); Xuecheng Xu (Zhejiang University); Yue Wang (Zhejiang University)*; Rong Xiong (Zhejiang University) . Interactive Session . 2020-11-16, 12:30 - 13:00 PST | PheedLoop Session . Abstract . The crucial step for localization is to match the current observation to the map. When the two sensor modalities are significantly different, matching becomes challenging. In this paper, we present an end-to-end deep phase correlation network (DPCN) to match heterogeneous sensor measurements. In DPCN, the primary component is a differentiable correlation-based estimator that back-propagates the pose error to learnable feature extractors, which addresses the problem that there are no direct common features for supervision. In addition, it eliminates the exhaustive evaluation in some previous methods, improving efficiency. With the interpretable modeling, the network is light-weighted and promising for better generalization. We evaluate the system on both the simulation data and Aero-Ground Dataset which consists of heterogeneous sensor images and aerial images acquired by satellites or aerial robots. The results show that our method is able to match the heterogeneous sensor measurements, outperforming the comparative traditional phase correlation and other learning-based methods. Code is available at https://github.com/jessychen1016/DPCN. Video . Reviews and Rebuttal . Reviews &amp; Rebuttal . ",
    "url": "http://localhost:4000/paper_530/",
    "relUrl": "/paper_530/"
  },"8": {
    "doc": "Tuesday",
    "title": "Tuesday",
    "content": " ",
    "url": "http://localhost:4000/tuesday",
    "relUrl": "/tuesday"
  },"9": {
    "doc": "Wednesday",
    "title": "Wednesday",
    "content": " ",
    "url": "http://localhost:4000/wednesday",
    "relUrl": "/wednesday"
  }
}
