---
layout: page
title: "Relational Learning for Skill Preconditions"
subtitle: 
description:
permalink: /paper_178/
grand_parent: All Papers
parent: Monday
supp: 
code: 
youtubeId: 
---

# Relational Learning for Skill Preconditions

[<i class="fa fa-file-text-o" aria-hidden="true"></i> Paper PDF ](https://drive.google.com/file/d/1CPzLxi8YazDSvQI13t7WK5N9hMCTC5ry/view){: .btn .btn-blue } {% if page.supp %} [<i class="fa fa-file-text-o" aria-hidden="true"></i> Supplementary ]({{ page.supp }}){: .btn .btn-green } {% endif %} {% if page.code %} [<i class="fa fa-github" aria-hidden="true"></i> Code]({{ page.code }}){: .btn .btn-red }
{% endif %}

#### Authors
**Mohit Sharma (Carnegie Mellon University)*; Oliver Kroemer (Carnegie Mellon University)**

#### Interactive Session
*2020-11-16, 11:10 - 11:40 PST*

#### Abstract
To determine if a skill can be executed in any given environment, a robot needs to learn the preconditions for the skill. As robots begin to operate in dynamic and unstructured environments, these precondition models will need to generalize to variable number of objects with different shapes and sizes. In this work, we focus on learning precondition models for manipulation skills in unconstrained environments.
Our work is motivated by the intuition that many complex manipulation tasks, with multiple objects, can be simplified by focusing on less complex pairwise object relations. We propose an object-relation model that learns continuous representations for these pairwise object relations.  Our object-relation model is trained completely in simulation, and once learned, is used by a separate precondition model to predict skill preconditions for real world tasks. We evaluate our precondition model on 3 different manipulation tasks: sweeping, cutting, and unstacking.  We show that our approach leads to significant improvements in predicting preconditions for all 3 tasks, across objects of different shapes and sizes.

#### Video 

#### Reviews

#### Rebuttal
