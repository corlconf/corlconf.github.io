---
layout: page
title: "Learning to Walk in the Real World with Minimal Human Effort"
subtitle: 
description:
permalink: /paper_245/
grand_parent: All Papers
parent: Tuesday
supp: 
code: 
youtubeId: 
---

# Learning to Walk in the Real World with Minimal Human Effort

[<i class="fa fa-file-text-o" aria-hidden="true"></i> Paper PDF ](https://drive.google.com/file/d/1MagiekXKKgQ-_Mg6Vl1HAXbrAvPF3OnK/view){: .btn .btn-blue } {% if page.supp %} [<i class="fa fa-file-text-o" aria-hidden="true"></i> Supplementary ]({{ page.supp }}){: .btn .btn-green } {% endif %} {% if page.code %} [<i class="fa fa-github" aria-hidden="true"></i> Code]({{ page.code }}){: .btn .btn-red }
{% endif %}

#### Authors
**Sehoon Ha (Georgia Institute of Technology); Peng Xu (Google Inc); Zhenyu Tan (Google); Sergey Levine (UC Berkeley)*; Jie Tan (Google)**

#### Interactive Session
*2020-11-17, 11:50 - 12:20 PST*

#### Abstract
Reliable and stable locomotion has been one of the most fundamental challenges for legged robots. Deep reinforcement learning (deep RL) has emerged as a promising method for developing such control policies autonomously. In this paper, we develop a system for learning legged locomotion policies with deep RL in the real world with minimal human effort. The key difficulties for on-robot learning systems are automatic data collection and safety. We overcome these two challenges by developing a multi-task learning procedure and a safety-constrained RL framework. We tested our system on the task of learning to walk on three different terrains: flat ground, a soft mattress, and a doormat with crevices. Our system can automatically and efficiently learn locomotion skills on a Minitaur robot with little human intervention.

#### Video 

#### Reviews

#### Rebuttal
