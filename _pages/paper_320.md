---
layout: page
title: "Learning rich touch representations through cross-modal self-supervision"
subtitle: 
description:
permalink: /paper_320/
grand_parent: All Papers
parent: Wednesday
supp: https://drive.google.com/file/d/1h74GyJAaUZPmjLDek_DkDflL_-GaWCwj/view
code: 
youtubeId: 
---

# Learning rich touch representations through cross-modal self-supervision

[<i class="fa fa-file-text-o" aria-hidden="true"></i> Paper PDF ](https://drive.google.com/file/d/1GZUE4tIE0umlEe0w6XPi6HiCyhNQ9loR/view){: .btn .btn-blue } {% if page.supp %} [<i class="fa fa-file-text-o" aria-hidden="true"></i> Supplementary ]({{ page.supp }}){: .btn .btn-green } {% endif %} {% if page.code %} [<i class="fa fa-github" aria-hidden="true"></i> Code]({{ page.code }}){: .btn .btn-red }
{% endif %}

#### Authors
**Martina Zambelli (DeepMind)*; Yusuf Aytar (DeepMind); Francesco Visin (Google DeepMind); Yuxiang Zhou (DeepMind); Raia Hadsell (Deepmind)**

#### Abstract
The sense of touch is fundamental in several manipulation tasks, but rarely used in robot manipulation. In this work we tackle the problem of learning rich touch features from cross-modal self-supervision. We evaluate them identifying objects and their properties in a few-shot classification setting. Two new datasets are introduced using a simulated anthropomorphic robotic hand equipped with tactile sensors on both synthetic and daily life objects. Several self-supervised learning methods are benchmarked on these datasets, by evaluating few-shot classification on unseen objects and poses. Our experiments indicate that cross-modal self-supervision effectively improves touch representation, and in turn has great potential to enhance robot manipulation skills.

#### Video 

#### Reviews

#### Rebuttal
